{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:35:18.039672Z",
     "start_time": "2019-12-03T07:35:10.481350Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df_files = pd.read_excel('../data/patentData_80000_20180622.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:35:18.079542Z",
     "start_time": "2019-12-03T07:35:18.042327Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "stopwordfile = open('../dict/StopWords_CON.txt', 'r', encoding='utf-8')\n",
    "def getstopword():\n",
    "    w = set()\n",
    "    for line in stopwordfile:\n",
    "        line.strip().split('\\n')\n",
    "        w.add(line[:len(line)-1].strip())\n",
    "    return w\n",
    "\n",
    "stopwordset = getstopword()\n",
    "\n",
    "def cutWords(sentence):\n",
    "    word_list = jieba.cut(sentence)\n",
    "    res = ' '.join(word_list)\n",
    "    res = res.split(' ')\n",
    "    tempX = ''\n",
    "    for i in res:\n",
    "        if i not in stopwordset:\n",
    "            tempX+=i\n",
    "            tempX+=' '\n",
    "    return tempX.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:35:18.189006Z",
     "start_time": "2019-12-03T07:35:18.081316Z"
    }
   },
   "outputs": [],
   "source": [
    "df_files['all_sentence'] = df_files['标题'] + df_files['摘要'] + df_files['首项权利要求']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:38:09.233907Z",
     "start_time": "2019-12-03T07:35:18.190727Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def DropNan(sentence):\n",
    "    if sentence is np.nan:\n",
    "        return ''\n",
    "    return sentence\n",
    "\n",
    "df_files['all_sentence'] = df_files['all_sentence'].apply(DropNan)\n",
    "df_files['all_sentence'] = df_files['all_sentence'].apply(lambda x: x.replace('\\n', ' '))\n",
    "df_files['all_sentence'] = df_files['all_sentence'].apply(cutWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:38:09.242090Z",
     "start_time": "2019-12-03T07:38:09.236076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        电动 真空 镊子 电动 真空 镊子 壳体 壳体 电源 电源 接触 片 电机 串联 电机 旋转...\n",
       "1        核工业 钎焊 料 系 核工业 钎焊 料 特别 核工 业 反应堆 不锈钢 耐蚀 合金 高温 合...\n",
       "2        伴有 沉淀 分离 溶剂 莘取 级 效率高 适应性 强 伴有 沉淀 生 分离 沉淀 溶剂萃取 ...\n",
       "3        谷物 粗粒粉 精选 清粉机 一项 谷物 粗粒粉 精选 干 叠放 筛层 筛层 至少 一台 平衡...\n",
       "4        旁滤式 离心机 难 分离 细粒 粘性 固渣 高效 分离 旁滤式 离心机 该机 装有 滤饼 旁...\n",
       "                               ...                        \n",
       "79995    重组 干扰素 ω 非典型 肺炎 防治 药物 中 重组 干扰素 ω 防治 非典 型 肺炎 药物...\n",
       "79996    ERBB 基础 肿瘤 治疗 物 治疗 哺乳动物 特别 人体 肿瘤 物 ErbB 蛋 编码 E...\n",
       "79997    治疗 儿童 股 骨干 骨折 可调式 悬吊 牵引 架 治疗 儿童 股 骨干 骨折 可调式 悬吊...\n",
       "79998    混合 纱布 袖管 包扎 包扎 混合 纱布 袖管 包扎 混合 纱布 袖管 柔韧 伸缩性 纤维 ...\n",
       "79999    人工 食管 人工 食管 猪 主动脉 酶 解可 吸收 无损伤 羊肠线 缝制成 二个 管道 二个...\n",
       "Name: all_sentence, Length: 80000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files['all_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:38:28.180685Z",
     "start_time": "2019-12-03T07:38:28.177218Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words = df_files['all_sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:38:30.957039Z",
     "start_time": "2019-12-03T07:38:28.421990Z"
    }
   },
   "outputs": [],
   "source": [
    "word2id = {\n",
    "    'padding': 0,\n",
    "    'unknown': 1\n",
    "}\n",
    "id2word = {\n",
    "    0: 'padding',\n",
    "    1: 'unknown'\n",
    "}\n",
    "cnt = 2\n",
    "for line_words in all_words:\n",
    "    word_list = line_words.split(' ')\n",
    "    # print (word_list)\n",
    "    for word in word_list:\n",
    "        # print (word)\n",
    "        # print (word not in word2id.keys())\n",
    "        if word not in word2id.keys():\n",
    "            word2id[word] = cnt\n",
    "            id2word[cnt] = word\n",
    "            cnt += 1\n",
    "            # print (cnt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:39:01.461214Z",
     "start_time": "2019-12-03T07:39:01.355700Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump([word2id, id2word], open(\"../data/word2idandid2word.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:40:00.778079Z",
     "start_time": "2019-12-03T07:39:48.803830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/data/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "modelword2vec = Word2Vec.load('../model/patentTIandAB_300features_40minwords_10context.model')\n",
    "word2vec_matrix = np.zeros((len(word2id), 300))\n",
    "for key in id2word.keys():\n",
    "    if (id2word[key] in modelword2vec):\n",
    "        word2vec_matrix[key] = modelword2vec[id2word[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T07:40:02.777889Z",
     "start_time": "2019-12-03T07:40:00.780363Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(word2vec_matrix, open(\"../data/word2vec_matrix.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
